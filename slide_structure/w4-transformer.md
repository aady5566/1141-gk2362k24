Transformer 核心概念與應用原理


[slide]
h1: 深度學習 AI 的變革者: Transformer
h2:
• Transformer 是一種神經網路架構，它在 2017 年 的經典論文《Attention is All You Need》中首次提出。
• 它徹底改變了人工智慧處理資訊的方式，成為深度學習模型（Deep Learning）的首選架構。
• Transformer 的核心威力 來自於「自我注意力機制」（Self-Attention Mechanism），能夠一次處理序列中的所有資訊。

[slide]
h2: 機器學習、深度學習、Transformer, GenAI 的關係
(幫我畫一張 interactive html 清楚呈現 Machine Learning /DL/ Transformer /Gen AI 的關係)
• ML/DL (機器/深度學習)： Transformer 是一種深度學習架構。
• GenAI (生成式 AI)： Transformer 是當前主流 生成式 AI（如 ChatGPT、Google Gemini、Meta Llama）的核心骨架。


[slide]
h3: 「我不是**頻果**的粉絲，但我喜歡吃**頻果**」

[slide]
h1：AI 下一個詞的預測 (Next-Word Prediction)

[slide]
h1: AI 文字生成原理：機率接龍
• Transformer 模型的工作核心原則是：給定一段文字輸入，它會預測接下來最可能出現的下一個詞是什麼。
• 想像你輸入：「夕陽無限好，只是近黃昏， 我 ...」。模型會列出所有可能的詞彙，並給予各自的機率。

[slide]
（將以下的數學原理用 html 可以之 latex 呈現）
title: 數學原理：Logits 與 Softmax
* 模型首先計算出每個詞彙的 原始分數 (Logits)。Logits 代表了每個詞彙的可能性，但尚未正規化。
* Softmax 函數將原始分數轉換為機率分佈，確保所有選項的機率總和為1。
* 公式:Logit $z_i$ 轉換為機率 $P_i$： $$P_i = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}$$
• 轉換後，所有詞彙的機率總和為 1，就是下一個字詞

[slide]
h1 核心機制 — Query, Key, Value (QKV)
h2：QKV 類比：圖書館/網路搜尋
p: Q、K、V 的概念是為了在模型內部模擬一種「搜尋與比對」的程序。

[slide]
h4: Query (Q) [查詢]
* 提問的意圖。代表目前這個詞「想要知道什麼」。
* 像你在 Google 搜尋引擎輸入的 關鍵字。
h4: Key (K) [鍵值]
提供的資訊標籤。代表序列中所有其他詞彙「能提供什麼資訊」。
像搜尋結果中所有網頁的 標題或目錄。
h4: Value (V) [數值]
實際的內容。一旦 Q 找到匹配的 K，V 就是模型要提取的「內容」。
像網頁的 實際文章內容。


[slide]
h1: QKV 運作流程
（以下 QKV 請繪製一個流程圖）
 當模型處理一個 Query (Q)，它會將這個 Q 拿去與序列中所有的 Key (K) 進行比對，找出最相關的 Key，然後加權提取對應的 Value (V)。

[slide]
h1: QKV 數學原理與價值：從單一詞義到多面向向量
h4: 數學原理：線性轉換
（請以 html latex 呈現）
• 在 Transformer 中，每個輸入的詞彙都會先轉換成一個 嵌入向量 ($X$)。
• Q、K、V 向量是透過將這個初始嵌入向量 $X$ 分別乘以三個不同的 可學習權重矩陣 ($W_Q, W_K, W_V$) 得到的： $$Q = X W_Q$$ $$K = X W_K$$ $$V = X W_V$$
QKV 的價值
• QKV 的設計允許模型從同一個初始詞彙嵌入中，提取出三個不同面向的資訊。
• 這三個不同的轉換（權重矩陣）讓模型能夠更精確地定義「搜尋」的目標 (Q)、「被搜尋」的標籤 (K) 和「內容」的價值 (V)。


[slide]
h1: 自我注意力機制 (Self-Attention)
h2: 上下文的計算與重要性分配
h3: 讓每個詞都擁有「全局視野」
• 自我注意力機制是 Transformer 區塊的核心。它讓序列中的每個詞都能考量到序列中所有其他詞的資訊，來重新定義自己的意思。
• 目標：讓詞彙的表徵從單純的詞義，提升為 上下文相關的語義表徵。
（以下 latex 呈現）
數學原理：縮放點積注意力
• Transformer 採用的是「縮放點積注意力」（Scaled Dot-Product Attention）。
• 注意力分數計算 ( $Q K^T$ )： 將 Query 矩陣與 Key 矩陣的轉置相乘 (點積)，計算出所有詞彙之間的 相似度或相關性。
• 公式： $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$
數學符號
意義
白話解釋
$Q K^T$
點積相似度
查詢 (Q) 與所有標題 (K) 的比對結果。
$\sqrt{d_k}$
縮放因子
用於防止向量維度 $d_k$ 過大時，點積結果也變得太大，導致 Softmax 運算後梯度變得不穩定。
Softmax
正規化
將相似度分數轉換成總和為 1 的 加權機率分佈。
$\times V$
加權求和
根據機率權重，從 Value (內容) 中提取上下文資訊。


[slide]
自我注意力與上下文價值
標題：精確捕捉語義：讓「蘋果」不再只有一種意思
核心價值：創造豐富的上下文表徵
• 自我注意力機制最重要的功能，是將輸入詞彙的嵌入轉化為 具有豐富上下文資訊的表徵。
• 在 Transformer 中，模型可以根據句子環境，為同一個詞賦予不同的意義，解決了傳統模型無法處理的一詞多義問題。
案例：語義的整合
• 句子 1： 「我喜歡吃 蘋果 。」
• 句子 2： 「他買了一台 蘋果 手機。」
• 運作過程：
    1. 當 Query (Q) 為「蘋果」時，它會與序列中的 Key (K)（例如：「吃」或「手機」）進行比對。
    2. 如果 Q-K 比對結果顯示它與「手機」高度相關，模型會從 Value (V) 提取「品牌/科技產品」的上下文資訊。
• 結果： 輸出的新向量 不再是單純的詞義，而是這個詞在當前句子中的精確語義。這種加權平均的機制使模型能精準捕捉長距離的依賴性。

[slide]
投影片 7：進階概念：多頭注意力 (Multi-Head Attention)
標題：多角度分析 — 讓模型像多位專家一樣思考
白話文：同時從不同角度看問題
• Transformer 不只使用一個自注意力機制，而是同時使用多個「注意力頭」（Head）平行運作。
• 想像你在看書，你需要多位專家同時幫助你理解：
    ◦ 頭 A： 專注於 文法結構（句法關係）。
    ◦ 頭 B： 專注於 語意關聯（詞彙意義）。
    ◦ 頭 C： 專注於 邏輯順序（上下文連貫性）。
核心原理
• 每個注意力頭都會使用 獨立學習的 Q、K、V 權重矩陣。
• 因此，每個頭能夠從輸入中捕捉 不同類型、不同層次 的關係。
• 所有注意力頭的結果會被 串接（Concatenate）起來，再整合成一個統一的、更豐富的表徵，極大提升模型的表達能力。


[slide]
穩定訓練機制 — 殘差連接 (Residual Connections)
標題：深層學習的「捷徑」與記憶力
白話文：防止資訊在深層網路中遺失
• 殘差連接（Residual Connections）是深度神經網路中的關鍵創新。
• 它就像在每一層（Transformer Block）都設置了一個「捷徑」（Shortcut）。
• 在處理資料時，殘差連接會將 未經當前層修改的原始輸入 直接跳過一或多層，並加到該層的最終輸出上。
數學與用途
• 核心功能： 確保「原始資訊」可以 無損地 流經多層網路。
• 穩定性： 有效減輕深度網路訓練中常見的「梯度消失問題」，確保模型底層的權重在訓練時能獲得足夠的更新。這對擁有 12 個或更多區塊的深層 Transformer 模型至關重要。

[slide]
控制創造力的「溫度」— 超參數 Temperature
標題：Temperature 決定模型輸出的確定性或隨機性
白話文：隨機性或創造力的「調節旋鈕」
• Temperature (T) 是一個超參數，用於控制模型在預測下一個詞時的 隨機性或創造力。
• 它的數值決定了機率分佈的 形狀。
出現的位置
• Temperature 出現在模型計算出 Logits 之後、Softmax 函數轉換成機率之前。

（以下 latex 呈現）
• 模型會將 Logits ($z_i$) 先除以 Temperature ($T$)，然後才進行 Softmax 運算： $$\text{Probability} = \text{Softmax}\left(\frac{\text{Logits}}{T}\right)$$ $$P_i = \frac{e^{z_i / T}}{\sum_{j=1}^{N} e^{z_j / T}}$$

[slide]
標題：超參數「溫度」 低溫求穩，高溫求變
Temperature $T$
特性/模式
對機率分佈的影響
輸出結果
應用情境
低溫 (T < 1, e.g., 0.2)
決定性 (Deterministic)
分佈更「尖銳」。差異被放大。
傾向選擇最高機率詞彙。結果 可預測且一致。
摘要、翻譯、事實型問答 (需要精度)。
高溫 (T > 1, e.g., 1.5)
隨機性/創造性 (Probabilistic/Creative)
分佈更「平滑」。差異被壓縮。
增加選擇低機率詞的機會。輸出更具 多樣性與驚喜。
故事寫作、頭腦風暴 (需要創意)。
• 極端情況： 當 $T=0$ 時，模型進入 貪婪模式 (Greedy Selection)，總是選擇最高機率的詞彙。
• 風險： 雖然高 $T$ 帶來創意，但也可能導致輸出不連貫、不準確或不符合語義。



[slide]
核心問題 — 自我注意力與人類注意力的差異？
標題：AI 的運算與人腦的思考
關鍵問題：效能與價值的權重分配
如果 Transformer 的效能取決於它如何分配注意力來決定「上下文價值」，那麼我們個人效能的提升，是否也遵循類似的「自我注意力」機制，精準地決定我們當下行動或決策中「價值 (Value)」的權重？

[slide]
差異比較 — 人機注意力的根本不同 (四個面向)
標題：AI 的平行運算 vs. 人腦的生物學限制
比較面向
人類注意力 (生物系統)
Transformer 自我注意力 (人工系統)
1. 容量限制與處理方式
受限/序列式。必須在不同輸入間切換。是解決 有限資源 的方案。
平行處理。在充足資源下，可以一次性考慮序列中所有元素。是一種提取 上下文關係 的機制。
2. 注意力路徑
雙向 (Top-down & Bottom-up)。受意圖、知識 (Top-down) 和顯著刺激 (Bottom-up) 驅動。
單向 (數據驅動)。注意力分配完全基於訓練數據的模式。缺乏由目標或認知狀態驅動的「由上而下」控制機制。
3. 意圖性與能動性
主動/有意識。注意力是一種主動的決策機制，與個體的 能動性 (Agency) 相關。
被動/數學構造。模型不具備內在的 認知狀態或意圖。其「注意力」本質上是計算元素間關係的數學構造。

[slide]
標題：大腦的「有限資源管理器」
人類注意力的定義與功能
• 定義： 注意力是心靈選擇性地掌握「幾個同時可能的對象或思緒」之一的能力。
• 核心功能： 這是對 有限的認知資源（limited computational resources）進行彈性控制（flexible control）的關鍵。
• 五大面向： 人類注意力包括了 選擇性注意力 (維持原有行為並過濾刺激)、分開性注意力 (同時專注多件事情)、轉移性注意力 (快速切換焦點)、持續性注意力 (長時間專注)、以及 集中的注意力 (井然有序地應對外部刺激)。
人類注意力的能力上限與限制
• 容量限制 (Capacity Constraints)： 人類受限於有限的視覺範圍和 工作記憶容量。工作記憶的限制類似於著名的「神奇的數字 4」 (Magic Number 4)。
• 處理方式： 由於這些生物限制，人類的注意力必須是 序列式 地運作，需要不斷切換焦點。
• 數據佐證： 類似於人類，Transformer 模型在 N-back 任務中，隨著 N 增加，準確度會顯著下降。研究假設這與自我注意力機制的容量限制相似。

[slide]
h1: 玩個遊戲！ magic 4



[slide]
文獻參考: TBD
參考文獻 Title (英文)
核心概念
經典連結


[slide]
h1：作業：用 QKV 分析你的「週末學習計畫」
白話文：我們如何為行動賦予價值？ 我們可以將 Transformer 的 Q-K-V 機制應用於個人決策，理解我們如何分配注意力來提取最有價值的行動。
情境：Query (Q) = 週末的學習意圖 | 決策要素 | 對應 Transformer 概念 | 範例內容 (Value, V) | 注意力分數 (Q $\cdot$ K) | 提取的上下文價值 (Output) | | :--- | :--- | :--- | :--- | :--- | | 你的意圖 (Q) | Query (Q)：你的提問 | 「我如何平衡課業與社交時間？」 | N/A | N/A | | 潛在資訊 (K) | Key (K)：可供參考的選項/經驗 | K1： 深度工作 | 高 (0.8) | V1： 95% 效率 | | | | K2： 聽音樂 | 中 (0.5) | V2： 50% 效率 | | | | K3： 滑手機/聊天 | 低 (0.1) | V3： 0% 效率 |
• 輸出結果 (加權總和)： 你的最終結果（例如，綜合效率值）是所有 Value 乘以其對應的注意力權重後求和。
• 啟示： 即使「滑手機」 (K3) 存在，但如果你將注意力權重分配得很低，你提取的實際價值 (V3) 就很少，最終的效率 (Output) 仍會很高。這就是自我注意力機制如何在生活中決定「上下文價值」。