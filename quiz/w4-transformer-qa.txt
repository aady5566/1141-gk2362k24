### Transformer 測驗題庫（基礎版 10 題，答案分散調整後）

---

**第1題**
Transformer 架構的最大創新是什麼？
a. 引入循環神經網路處理長序列
b. 使用卷積層取代嵌入向量
c. 自我注意力機制，可同時處理整個序列
d. 減少參數，避免過擬合

正確答案：c
解析：Transformer 的核心創新是自我注意力機制，讓模型在處理每個詞時能同時考量整個序列，擺脫了 RNN 的逐步限制。

---

**第2題**
以下哪一項是 Transformer 的錯誤特性？
a. 可以同時處理整個序列中的所有詞
b. 使用自我注意力機制捕捉長距離依賴
c. 需要透過訓練學習 Q、K、V 的權重矩陣
d. 必須逐步處理序列，無法並行計算

正確答案：d
解析：逐步處理序列是 RNN 的限制，而 Transformer 最大的優勢之一就是能夠並行計算，並同時考量序列中所有詞。

---

**第3題**
在詞彙機率預測過程中，Logits 的角色是什麼？
a. 模型計算出的未正規化分數
b. 機率總和為 1 的分佈
c. 輸入的嵌入向量
d. 注意力權重

正確答案：a
解析：Logits 是模型在 Softmax 之前的原始分數，之後經 Softmax 轉換為機率分佈。

---

**第4題**
Softmax 在 Transformer 中的功能是什麼？
a. 將分數壓縮到 -1 到 1 之間
b. 將所有輸入向量轉換為 QKV
c. 將 Logits 正規化為機率分佈
d. 增加模型的計算速度

正確答案：c
解析：Softmax 將 Logits 轉換為 0 到 1 之間的機率分佈，且總和為 1。

---

**第5題**
Q、K、V 在 Transformer 中的對應意義為？
a. 查詢、鍵值、數值
b. 問題、答案、結果
c. 向量、矩陣、張量
d. 查詢、分類、值

正確答案：a
解析：Q=Query（查詢）、K=Key（鍵值）、V=Value（數值），分別代表搜尋意圖、資訊標籤與實際內容。

---

**第6題**
在注意力運算中，為什麼要對「查詢與鍵值的相似度分數」進行縮放處理？
a. 減少參數數量
b. 提高模型容量
c. 確保 Softmax 的輸入數值不會過大而導致梯度不穩定
d. 減少訓練時間

正確答案：c
解析：若相似度分數過大，Softmax 的輸出會極端偏向單一值，導致梯度不穩定。縮放處理能讓數值控制在合理範圍。

---

**第7題**
多頭注意力機制的主要優勢是什麼？
a. 從多角度捕捉不同語意關係
b. 降低模型參數
c. 提升計算速度
d. 減少訓練資料需求

正確答案：a
解析：多頭注意力讓不同「頭」專注於不同關係（文法、語意、邏輯），最後再整合成豐富的表徵。

---

**第8題**
殘差連接 (Residual Connection) 在 Transformer 中的主要功能是？
a. 提升隨機性
b. 減少運算量
c. 替代多頭注意力
d. 確保資訊暢通，避免梯度消失

正確答案：d
解析：殘差連接讓原始資訊直接跨層傳遞，避免在深層網路中丟失，並有助於穩定訓練。

---

**第9題**
Temperature 參數調高 (T > 1) 時，模型輸出會如何？
a. 更隨機、多樣化
b. 更保守、重複性高
c. 完全不輸出文字
d. 變成確定性預測

正確答案：a
解析：高溫使機率分佈更平滑，低機率詞被選中的機會上升，輸出更具創造性但可能失去連貫性。

---

**第10題**
下列哪一項正確描述 Transformer 與人類注意力的差異？
a. Transformer 有內在意圖，人類沒有
b. 人類注意力可同時平行處理無限資訊，Transformer 無法
c. Transformer 注意力由數據驅動，人類注意力有意識主導
d. 兩者運作方式完全相同

正確答案：c
解析：Transformer 的注意力分配是數學運算結果，人類注意力則結合了意圖、意識與外部刺激的控制。
